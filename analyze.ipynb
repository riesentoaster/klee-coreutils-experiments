{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Any, Dict\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from natsort import natsorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global information about each suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = {\n",
    "    run: {\"version\": version.name}\n",
    "    for version in Path(\"./out\").iterdir()\n",
    "    for run in version.iterdir()\n",
    "}\n",
    "\n",
    "\n",
    "def add_entry_to_run(\n",
    "    new_key: str, f: Callable[[Path, Dict[str, Any]], Any]\n",
    ") -> Dict[Path, Dict[str, Any]]:\n",
    "    global runs\n",
    "    for key, value in runs.items():\n",
    "        value.update({new_key: f(key, value)})\n",
    "\n",
    "\n",
    "add_entry_to_run(\"name\", lambda p, d: p.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time(p:Path, d):\n",
    "    file_path = p.rglob(\"klee/info\").__next__()\n",
    "    if file_path.exists():\n",
    "        with open(file_path, \"r\") as file:\n",
    "            res = re.search(r\"--max-time=(\\w*)\", file.read())\n",
    "            if res:\n",
    "                return res.group(1)\n",
    "    else:\n",
    "        print(f\"Error for {p}\")\n",
    "        return None\n",
    "\n",
    "add_entry_to_run(\"time\", get_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per util information\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_entry_to_run(\"df\", lambda p, d: pd.DataFrame(columns=[e.name for e in p.iterdir() if e.is_dir()]))\n",
    "\n",
    "def add_entry_for_utils(key: str, f: Callable[[Path], Any]) -> None:\n",
    "    \"\"\"\n",
    "    Add entry for all utils\n",
    "\n",
    "    Paramenters:\n",
    "    key (str): key to add the new value at in the dataframe\n",
    "    f (Callable[[Path], Any]): function taking the path to the subfolder for the util and returning the appropriate value\n",
    "    \"\"\"\n",
    "\n",
    "    def adder(p: Path, d):\n",
    "        df = d[\"df\"]\n",
    "        res = {}\n",
    "        for util in df.columns:\n",
    "            path = p / util\n",
    "            if not path.exists():\n",
    "                raise Exception(f\"Path \\\"{util}\\\" does not exist\")\n",
    "            res[util] = f(path)\n",
    "        df.loc[key] = res\n",
    "        return df\n",
    "    add_entry_to_run(\"df\", adder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of errors according to KLEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for out/coreutils-8.25/1h-3/hostname\n",
      "Error for out/coreutils-8.25/1h-3/setuidgid\n",
      "Error for out/coreutils-8.25/1h-2/hostname\n",
      "Error for out/coreutils-8.25/1h-2/setuidgid\n",
      "Error for out/coreutils-8.25/1h/hostname\n",
      "Error for out/coreutils-8.25/1h/setuidgid\n",
      "Error for out/coreutils-9.4/1h/hostname\n",
      "Error for out/coreutils-9.4/1h/setuidgid\n"
     ]
    }
   ],
   "source": [
    "def read_num_errors(util_path: Path) -> str:\n",
    "    file_path = util_path / \"klee\"\n",
    "    if file_path.exists():\n",
    "        return str(len(list(file_path.glob(\"*.err\"))))\n",
    "    else:\n",
    "        print(f\"Error for {util_path}\")\n",
    "        return None\n",
    "\n",
    "add_entry_for_utils(\"num_errors\", read_num_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage according to KLEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for ICov(%) — out/coreutils-8.25/1h-3/hostname\n",
      "Error for ICov(%) — out/coreutils-8.25/1h-3/setuidgid\n",
      "Error for ICov(%) — out/coreutils-8.25/1h-2/hostname\n",
      "Error for ICov(%) — out/coreutils-8.25/1h-2/setuidgid\n",
      "Error for ICov(%) — out/coreutils-8.25/1h/hostname\n",
      "Error for ICov(%) — out/coreutils-8.25/1h/setuidgid\n",
      "Error for ICov(%) — out/coreutils-9.4/1h/hostname\n",
      "Error for ICov(%) — out/coreutils-9.4/1h/setuidgid\n",
      "Error for BCov(%) — out/coreutils-8.25/1h-3/hostname\n",
      "Error for BCov(%) — out/coreutils-8.25/1h-3/setuidgid\n",
      "Error for BCov(%) — out/coreutils-8.25/1h-2/hostname\n",
      "Error for BCov(%) — out/coreutils-8.25/1h-2/setuidgid\n",
      "Error for BCov(%) — out/coreutils-8.25/1h/hostname\n",
      "Error for BCov(%) — out/coreutils-8.25/1h/setuidgid\n",
      "Error for BCov(%) — out/coreutils-9.4/1h/hostname\n",
      "Error for BCov(%) — out/coreutils-9.4/1h/setuidgid\n"
     ]
    }
   ],
   "source": [
    "def read_klee_csv(csv_name: str) -> Callable[[Path], str]:\n",
    "    def f(util_path: Path) -> str:\n",
    "        file_path = util_path / \"klee-stats.csv\"\n",
    "        if file_path.exists() and file_path.stat().st_size > 0:\n",
    "            df = pd.read_csv(file_path)\n",
    "            return str(df[csv_name][0])\n",
    "        else:\n",
    "            print(f\"Error for {csv_name} — {util_path}\")\n",
    "            return None\n",
    "    return f\n",
    "\n",
    "add_entry_for_utils(\"klee_ICov\", read_klee_csv(\"ICov(%)\"))\n",
    "add_entry_for_utils(\"klee_BCov\", read_klee_csv(\"BCov(%)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage according to `gcov`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for out/coreutils-8.25/1h-3/hostname\n",
      "Error for out/coreutils-8.25/1h-3/setuidgid\n",
      "Error for out/coreutils-8.25/1h-2/hostname\n",
      "Error for out/coreutils-8.25/1h-2/setuidgid\n",
      "Error for out/coreutils-8.25/1h/hostname\n",
      "Error for out/coreutils-8.25/1h/setuidgid\n",
      "Error for out/coreutils-6.10/6h-3/uniq\n",
      "Error for out/coreutils-6.10/6h-3/who\n",
      "Error for out/coreutils-6.10/6h-3/tsort\n",
      "Error for out/coreutils-9.4/1h/hostname\n",
      "Error for out/coreutils-9.4/1h/setuidgid\n",
      "Error for out/coreutils-9.4/1h/cksum\n",
      "Error for out/coreutils-9.4/1h/wc\n"
     ]
    }
   ],
   "source": [
    "def read_gcov_cov(util_path: Path) -> str:\n",
    "    file_path = util_path / \"cov.txt\"\n",
    "    if file_path.exists():\n",
    "        with open(file_path, \"r\") as file:\n",
    "            res = re.search(r\"File '(\\.\\./)?\\.\\./src/(\\w+)\\.c'\\nLines executed:(\\d?\\d\\d.\\d\\d)% of \\d+\", file.read())\n",
    "            if res:\n",
    "                return res.group(3)\n",
    "    else:\n",
    "        print(f\"Error for {util_path}\")\n",
    "        return None\n",
    "\n",
    "add_entry_for_utils(\"gcov_cov\", read_gcov_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n",
    "### Massaging `df`s together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             key      util  value      run     time         version\n",
      "2562   klee_ICov      echo  36.64     1h-3    60min  coreutils-6.10\n",
      "2167  num_errors     chmod   0.00     6h-2   360min  coreutils-6.10\n",
      "4585   klee_ICov      kill  45.07  10min-3    10min  coreutils-6.10\n",
      "1911   klee_BCov       who  22.45      24h  1440min  coreutils-6.10\n",
      "268     gcov_cov     tsort  96.24     1h-3    60min  coreutils-8.25\n",
      "1677    gcov_cov     pinky  55.67     6h-3   360min  coreutils-6.10\n",
      "4948    gcov_cov       cut  83.08       1h    60min  coreutils-6.10\n",
      "1415   klee_ICov    chroot  38.62     6h-3   360min  coreutils-6.10\n",
      "3016   klee_ICov    uptime  42.09       6h   360min  coreutils-6.10\n",
      "1598   klee_BCov        nl  26.39     6h-3   360min  coreutils-6.10\n",
      "620     gcov_cov      stty  79.61     1h-2    60min  coreutils-8.25\n",
      "4446  num_errors    md5sum   1.00    10min    10min  coreutils-6.10\n",
      "204    klee_BCov     cksum  27.54     1h-3    60min  coreutils-8.25\n",
      "1921  num_errors        mv   0.00      24h  1440min  coreutils-6.10\n",
      "1902   klee_ICov    runcon  36.40      24h  1440min  coreutils-6.10\n",
      "3524   klee_BCov     mknod  29.35     1h-4    60min  coreutils-6.10\n",
      "913     gcov_cov        du  65.98       1h    60min  coreutils-8.25\n",
      "5228   klee_ICov        tr  41.53       1h    60min  coreutils-6.10\n",
      "3905   klee_ICov  readlink  42.22    24h-2  1440min  coreutils-6.10\n",
      "3244  num_errors      tail   0.00     1h-2    60min  coreutils-6.10\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for k, v in runs.items():\n",
    "    df = v[\"df\"]\n",
    "    df = df.reset_index(names=\"key\")\n",
    "    df = df.melt(id_vars=\"key\", var_name=\"util\")\n",
    "    # .melt(id_vars=\"\")\n",
    "    df[\"run\"] = k.name\n",
    "    df[\"time\"] = v[\"time\"]\n",
    "    df[\"version\"] = v[\"version\"]\n",
    "    dfs.append(df)\n",
    "\n",
    "combined_df = pd.concat(dfs)\n",
    "combined_df['value'] = combined_df['value'].astype(np.float64)\n",
    "combined_df = combined_df.dropna(subset=['value'])\n",
    "combined_df = combined_df.reset_index(drop=True)\n",
    "print(combined_df.sample(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots by coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_df = combined_df\n",
    "versions = natsorted(coverage_df[\"version\"].unique())\n",
    "keys = natsorted(coverage_df[\"key\"].unique())\n",
    "time_categories = natsorted(coverage_df[\"time\"].unique())\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=len(keys),\n",
    "    ncols=len(versions),\n",
    "    figsize=(10 * len(versions), 5 * len(keys)),\n",
    "    dpi=300,\n",
    ")\n",
    "fig.suptitle(f\"Empirical Cumulative Distribution Function (ECDF)\", fontsize=20, y=0.99)\n",
    "color_map = dict(zip(time_categories, sns.color_palette(n_colors=len(time_categories))))\n",
    "\n",
    "for version_i, version in enumerate(versions):\n",
    "    version_df = coverage_df[coverage_df[\"version\"] == version].drop(columns=\"version\")\n",
    "    for key_i, key in enumerate(keys):\n",
    "        key_df = version_df[version_df[\"key\"] == key].drop(columns=\"key\")\n",
    "        ax = axes[key_i, version_i]\n",
    "        ax.set_title(f\"{key} — {version}\")\n",
    "        for time in natsorted(key_df[\"time\"].unique()):\n",
    "            time_df = key_df[key_df[\"time\"] == time].drop(columns=\"time\")\n",
    "            for run_i, run in enumerate(time_df[\"run\"].unique()):\n",
    "                run_df = time_df[time_df[\"run\"] == run].drop(columns=\"run\")\n",
    "                sns.ecdfplot(\n",
    "                    y=\"value\",\n",
    "                    data=run_df,\n",
    "                    ax=ax,\n",
    "                    color=color_map[time],\n",
    "                    label=time if run_i == 0 else \"_nolegend_\",\n",
    "                    stat=\"count\",\n",
    "                )\n",
    "        ax.legend(title=\"Time\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"plots/ecdf.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gains by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combined_df[combined_df[\"version\"] == \"coreutils-6.10\"].drop(\n",
    "    columns=[\"run\", \"version\"]\n",
    ")\n",
    "df = df.groupby([\"key\", \"time\", \"util\"], as_index=False).mean()\n",
    "keys = natsorted(df[\"key\"].unique())\n",
    "fig, axes = plt.subplots(nrows=len(keys), ncols=1, figsize=(5, 3 * len(keys)), dpi=300)\n",
    "for i, key in enumerate(keys):\n",
    "    df_keys = df[df[\"key\"] == key]\n",
    "    df_keys = df_keys.drop(columns=\"key\")\n",
    "    order = natsorted(df_keys[\"time\"].unique())\n",
    "    df_keys[\"time\"] = pd.Categorical(df_keys[\"time\"], categories=order, ordered=True)\n",
    "    df_keys = df_keys.sort_values([\"util\", \"time\"])\n",
    "    df_keys[\"difference\"] = df_keys.groupby(\"util\", as_index=False)[\"value\"].diff()\n",
    "    df_keys = df_keys.drop(columns=[\"util\", \"value\"])\n",
    "    df_keys = df_keys.groupby([\"time\"], as_index=False, observed=True).mean()\n",
    "    df_keys[\"time\"] = (\n",
    "        df_keys[\"time\"].shift(1).astype(str) + \" - \" + df_keys[\"time\"].astype(str)\n",
    "    )\n",
    "    df_keys = df_keys.dropna()\n",
    "    sns.barplot(data=df_keys, x=\"time\", y=\"difference\", ax=axes[i])\n",
    "    axes[i].set_ylabel(f\"average {key} gained\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"plots/gains_by_time.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots by util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gcov_cov\n",
      "Done with klee_BCov\n",
      "Done with klee_ICov\n",
      "Done with num_errors\n"
     ]
    }
   ],
   "source": [
    "def paint_util(args):\n",
    "    key, util, key_df = args\n",
    "    util_df = key_df[key_df[\"util\"] == util].drop(columns=\"util\")\n",
    "    util_df = util_df.sort_values(by=\"version\")\n",
    "    times = natsorted(util_df['time'].unique())\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6), dpi=300)\n",
    "    sns.stripplot(data=util_df, ax=ax, x='time', y=\"value\", hue=\"version\", order=times)\n",
    "    ax.set_title(f\"{util}\")\n",
    "    ax.set_ylabel(key)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"plots/{key}/{util}.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "for key in natsorted(combined_df[\"key\"].unique()):\n",
    "    key_df = combined_df[combined_df[\"key\"] == key].drop(columns=\"key\")\n",
    "    Path(f\"plots/{key}\").mkdir(exist_ok=True)\n",
    "    \n",
    "    utils = natsorted(key_df[\"util\"].unique())\n",
    "\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        pool.map(paint_util, [(key, util, key_df) for util in utils])\n",
    "    \n",
    "    print(f\"Done with {key}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
