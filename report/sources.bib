@inproceedings{KLEE,
  author    = {Cadar, Cristian and Dunbar, Daniel and Engler, Dawson},
  title     = {KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs},
  year      = {2008},
  publisher = {USENIX Association},
  address   = {USA},
  abstract  = {We present a new symbolic execution tool, KLEE, capable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment installed on millions of Unix systems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage -- on average over 90\% per tool (median: over 94\%) -- and significantly beat the coverage of the developers' own hand-written test suite. When we did the same for 75 equivalent tools in the BUSYBOX embedded system suite, results were even better, including 100\% coverage on 31 of them.We also used KLEE as a bug finding tool, applying it to 452 applications (over 430K total lines of code), where it found 56 serious bugs, including three in COREUTILS that had been missed for over 15 years. Finally, we used KLEE to crosscheck purportedly identical BUSYBOX and COREUTILS utilities, finding functional correctness errors and a myriad of inconsistencies.},
  booktitle = {Proceedings of the 8th USENIX Conference on Operating Systems Design and Implementation},
  pages     = {209–224},
  numpages  = {16},
  location  = {San Diego, California},
  series    = {OSDI'08}
}

@online{KLEEWebsite,
  title   = {{KLEE} Symbolic Execution Engine},
  year    = {2024},
  url     = {https://klee.github.io},
  urldate = {2024-01-24}
}

@online{KLEEFAQ,
  title   = {{OSDI}'08 Coreutils Experiments},
  year    = {2024},
  url     = {https://klee.github.io/docs/coreutils-experiments/},
  urldate = {2024-01-24}
}

@online{KLEETutorial,
  title   = {Tutorial on How to Use {KLEE} to Test {GNU} Coreutils},
  year    = {2024},
  url     = {https://klee.github.io/tutorials/testing-coreutils/},
  urldate = {2024-01-24}
}
@online{KLEEIssue,
  title   = {{KLEE}: {-O0} is not a recommended option for clang},
  year    = {2024},
  url     = {https://github.com/klee/klee/issues/902},
  urldate = {2024-01-24}
}

@online{STP,
  title   = {The Simple Theorem Prover},
  year    = {2024},
  url     = {http://stp.github.io},
  urldate = {2024-01-24}
}
@online{WLLVM,
  title   = {Whole Program LLVM},
  year    = {2024},
  url     = {https://github.com/travitch/whole-program-llvm},
  urldate = {2024-01-24}
}

@unpublished{EVA,
  author = {Valentin Huber},
  title  = {Challenges and Mitigation Strategies in Symbolic Execution Based Fuzzing Through the Lens of Survey Papers},
  year   = {2023},
  month  = {12},
  day    = {15},
  url    = {https://github.com/riesentoaster/review-symbolic-execution-in-fuzzing/releases/download/v1.0/Huber-Valentin-Challenges-and-Mitigation-Strategies-in-Symbolic-Execution-Based-Fuzzing-Through-the-Lens-of-Survey-Papers.pdf}
}

@mastersthesis{BA,
  author = {Flum, Silvan and Huber, Valentin},
  title  = {Ghidrion: A Ghidra Plugin to Support Symbolic Execution},
  school = {Zürich University of Applied Science — Institute of Applied Information Technology},
  type   = {Bachelor's Thesis},
  year   = {2023},
  month  = {06},
  day    = {09},
  url    = {https://valentinhuber.me/assets/ghidrion.pdf}
}

@inproceedings{UNIFUZZ,
  author    = {Yuwei Li and Shouling Ji and Yuan Chen and Sizhuang Liang and Wei-Han Lee and Yueyao Chen and Chenyang Lyu and Chunming Wu and Raheem Beyah and Peng Cheng and Kangjie Lu and Ting Wang},
  title     = {{UNIFUZZ}: A Holistic and Pragmatic {Metrics-Driven} Platform for Evaluating Fuzzers},
  booktitle = {30th USENIX Security Symposium (USENIX Security 21)},
  year      = {2021},
  isbn      = {978-1-939133-24-3},
  pages     = {2777--2794},
  url       = {https://www.usenix.org/conference/usenixsecurity21/presentation/li-yuwei},
  publisher = {USENIX Association},
  month     = aug
}

@inproceedings{EvaluatingFuzzTesting,
  author    = {Klees, George and Ruef, Andrew and Cooper, Benji and Wei, Shiyi and Hicks, Michael},
  title     = {Evaluating Fuzz Testing},
  year      = {2018},
  isbn      = {9781450356930},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3243734.3243804},
  doi       = {10.1145/3243734.3243804},
  abstract  = {Fuzz testing has enjoyed great success at discovering security critical bugs in real software. Recently, researchers have devoted significant effort to devising new fuzzing techniques, strategies, and algorithms. Such new ideas are primarily evaluated experimentally so an important question is: What experimental setup is needed to produce trustworthy results? We surveyed the recent research literature and assessed the experimental evaluations carried out by 32 fuzzing papers. We found problems in every evaluation we considered. We then performed our own extensive experimental evaluation using an existing fuzzer. Our results showed that the general problems we found in existing experimental evaluations can indeed translate to actual wrong or misleading assessments. We conclude with some guidelines that we hope will help improve experimental evaluations of fuzz testing algorithms, making reported results more robust.},
  booktitle = {Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
  pages     = {2123–2138},
  numpages  = {16},
  keywords  = {security, fuzzing, evaluation},
  location  = {Toronto, Canada},
  series    = {CCS '18}
}