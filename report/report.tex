\documentclass{article}
\usepackage[hidelinks]{hyperref}
\usepackage{csquotes}
\usepackage[vmargin=25mm, hmargin=20mm]{geometry}
\usepackage{amsmath}  % for \hookrightarrow
\usepackage{xcolor}   % for \textcolor
\usepackage{listings}
\lstset{
    backgroundcolor=\color[RGB]{240, 240, 240},   
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    %    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space}
}

\lstdefinelanguage{docker}{
  keywords={FROM, RUN, COPY, ADD, ENTRYPOINT, CMD,  ENV, ARG, WORKDIR, EXPOSE, LABEL, USER, VOLUME, STOPSIGNAL, ONBUILD, MAINTAINER, HEALTHCHECK},
  keywordstyle=\color{blue}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{\#},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]"
}



% \usepackage{longtable}
% \usepackage{array}
\usepackage[
    backend=biber,
    sorting=none,
    style=ieee,
    urldate=long,
    maxcitenames=2,
    mincitenames=1
]{biblatex}
\addbibresource{sources.bib}
\usepackage{multicol}
\setlength{\columnsep}{13mm}
% \usepackage{float}
% \usepackage{graphicx}

\let\savedCite=\cite
\renewcommand{\cite}{\unskip~\savedCite}
\let\savedRef=\ref
\renewcommand{\ref}{\unskip~\savedRef}
\hfuzz=50px
\hbadness=10000


\title{Running KLEE on GNU coreutils}

\begin{document}

\author{%
    Valentin Huber\vspace{5px}\\%
    \small \href{https://www.zhaw.ch/en/engineering/institutes-centres/init/}{Institute of Applied Information Technology}\\%
    \small \href{https://www.zhaw.ch/en}{ZÃ¼rich University of Applied Sciences ZHAW}\\%
    \small \href{mailto://contact@valentinhuber.me}{contact@valentinhuber.me}%
    \vspace{10px}
}

\date{\today\vspace{5px}}

\maketitle
\begin{multicols}{2}
    \tableofcontents

    \section{Introduction}
    KLEE\cite{KLEEWebsite} is an open source, symbolic execution based, advanced fuzzing platform. It was introduced in the seminal paper titled \citetitle{KLEE} in \citeyear{KLEE}. In their article, \citeauthor{KLEE} present their work and evaluate it on a diverse set of programs. The most prominent of those is the GNU coreutils suite, in which ten fatal errors were found.

    Ever since then, KLEE has not only matured as a fuzzer, it has also been used extensively as a platform for other researchers to build on top of, as I have discovered in\cite{EVA}. As an introduction to the practical side of fuzzing, I attempted to answer the following questions about KLEE:

    \begin{enumerate}
        \item Reproducing the original paper (see Section~\ref{Reproducing})
              \begin{enumerate}
                  \item Can the current version of KLEE be run on coreutils version 6.10 (as tested in the original paper)?
                  \item Can the same metrics as measured in the original paper still be measured?
                  \item How do the measured metrics compare to what was published 15 years ago?
              \end{enumerate}
        \item Examining the statistical distribution of results over different fuzzing times (see Section~\ref{Timeouts})
              \begin{enumerate}
                  \item How does the non-determinism in KLEE influence the variance in the results between different test runs?
                  \item How do different testing timeouts influence results?
              \end{enumerate}
        \item Testing more recent versions of coreutils (see Section~\ref{SoftwareEvolution})
              \begin{enumerate}
                  \item What needs to change in the test setup to test more recent versions of coreutils?
                  \item How do the results from testing different versions of coreutils differ?
              \end{enumerate}
    \end{enumerate}

    All experiments were run on a virtualized server with the following specs: AMD EPYC 7713 64C 225W 2.0GHz Processor, 1 TiB RAM, 2x 25GiB/s Ethernet.
    \section{Background}

    What follows is a short explanation of the application of symbolic execution in fuzzing. For more extensive background, I refer to some of my previous work\cite{EVA, BA}.

    Remember that KLEE is an open-source, symbolic execution based fuzzer. It takes LLVM bytecode from the program under test (PUT) as its input, runs its analysis on it. KLEE then outputs some statistics about the run, inputs to the PUT that crash it, and inputs that, when executed, cover all branches KLEE has examined during its analysis.

    \subsection{A Primer on Symbolic Execution}
    KLEE is a fuzzer based on symbolic execution. This means that instead of executing a PUT with a concrete value, it instead runs through the instructions and maps relationships between data in memory (such as variables and user input) to mathematical formulas. So an instruction like \lstinline[language=llvm]{%result = add i32 %a, %b} would be mapped to the logical relationship $\phi_k=\phi_i+\phi_j$. Conditional jumps are mapped to conditions on these variables for both outcomes of the condition, so the instruction \lstinline[language=llvm]{%isPositive = icmp sgt i32 %result, 0} would be represented with $\phi_k>0$ and $\phi_k\le0$ respectively.

    The set of all conditions along a certain path through the PUT are called the \textit{path condition}. It can be passed to s satisfiability modulo theories (SMT) solver (KLEE uses STP\cite{STP} as a default), which returns values for all user inputs, such that the PUT is forced down the exact path represented by the path condition.

    This is the major advantage of symbolic execution based fuzzing, as compared to ordinary fuzzing. By not using concrete values, but instead logical representations of user input, it essentially runs through the PUT with all possible user inputs \textit{at the same time}. So if the solver returns that no inputs satisfy the passed formula, we have proven that such inputs simply do not exist. To be able to do this, it accepts the huge overhead of translating the code to formulas and then solving them.

    \subsection{Symbolic Execution in Practice}
    Symbolic execution in fuzzing has several major challenges to overcome. I have previously discussed them in detail\cite{EVA}, but would like to give a short summary here:

    \begin{itemize}
        \item Environment interactions (such as file system interactions) in general are opaque to the fuzzer and cannot be mapped to logical formulas. KLEE deals with this by solving the path constraint before the instruction in question and then uses concrete values in the call. This abandons the claim on completeness symbolic execution typically has, but is often the only feasible way to still continue analysis.
        \item The second major challenge in symbolic execution is what is known as \textit{path explosion}. Because the number of program states grows exponentially with the number of instructions, for all but the most simple programs it is not feasible to calculate the entire state space. KLEE deals with this by reducing the search space to actually executable instructions, using advanced data structures, and examining paths through the PUT consecutively, with a user-defined timeout. To maximize the state space and code coverage as quickly as possible, it alternates between two strategies for choosing the next input to evaluate: KLEE either chooses the input that promises to increase the coverage the most and a random input to prevent the execution from getting stuck in a certain subtree of the PUT.
        \item KLEE needs to model the entire memory of a process. This is straight-forward as long as variables are used directly but becomes a challenge when pointers are involved. This is especially true if the value of these pointers depends on user input, since this would require KLEE to model all possible addresses having all possible values, which instantly explodes the memory consumption and number of states to examine and is thus infeasible. KLEE deals with this by representing such pointer operation as array accesses where the accessed object is copied as often as necessary to model all possible results, including error states.
        \item As programs become more complex, the path constraints become increasingly long and solving them contributes more and more to the fuzzer's runtime. KLEE applies some advanced optimizations, like query splitting and more general optimizations, or a cache of previous results, which often solve supersets the query they are a solution to. Finally, KLEE defines a timeout, after which the solver is interrupted and analysis is continued at an other branch.
    \end{itemize}

    \section{Reproducing the Original Paper}
    \label{Reproducing}
    I'm basing my experiment setup on the original paper\cite{KLEE}, the FAQs in the project's documentation\cite{KLEEFAQ} and the tutorial on testing coreutils version 6.11\cite{KLEETutorial}.

    \subsection{Project Setup}
    KLEE is a complex system including complex dependencies such as the SMT solvers. The maintainers provide a Dockerfile and the corresponding Docker image. Using Docker as an intermediate form of virtualization adds a layer of indirection and a performance penalty. However, since I'm not necessarily interested in maximizing performance in this project, but instead focus on comparing different setups, this is a tradeoff worth taking. It further makes complex build steps reproducible and acts as documentation.

    However, the Docker image provided at this time is based on Ubuntu 22 (Jammy), and no longer builds coreutils 6.10 with the GNU Compiler (GCC). This is because its build system checks attempts to detect what system it is running on, and the variable the detection is based on is no longer defined. Specifically, in \lstinline{freadahead.c} the following check is performed:

    \lstinputlisting[firstline=25, lastline=25, language=C, consecutivenumbers=false]{assets/freadahead.c}

    The error message and the full \lstinline{freadahead.c} can be found in Appendix~\ref{AppendixErrorBuildOldInNew}.

    \subsubsection{Using an Old Version of Ubuntu}
    \label{OldVersion}

    One attempt to mitigate this issue would be to rewrite this check to allow the version of GCC installed on KLEE's Docker image to compile coreutils 6.10. However, I opted to pursue a different avenue, because of two reasons:
    \begin{enumerate}
        \item Build systems are not my area of expertise and I do not know how many other issues would appear once the first was solved.
        \item Changing code always adds risk of introducing additional software errors, which would distort my findings.
    \end{enumerate}

    Therefore, I attempted to build the binaries on an old version of Ubuntu, and then move the binaries to KLEE's Docker image. Specifically, I chose the latest LTS version which was available when version 6.10 of coreutils was current. This approach worked without any additional changes to the code nor the build system. The setup of the Docker image then used to build coreutils can be seen in Listing~\ref{dockerfile1}.

\end{multicols}
\lstinputlisting[
    language=docker,
    lastline=21,
    consecutivenumbers=false,
    caption=Dockerfile content to prepare a system for building coreutils 6.10,
    label=dockerfile1
]{../Dockerfile}
\lstinputlisting[
    language=docker,
    firstline=23,
    lastline=54,
    consecutivenumbers=false,
    caption=Dockerfile content to build coreutils to LLVM bytecode using WLLVM,
    label=dockerfile2
]{../Dockerfile}
\begin{multicols}{2}

    \subsubsection{LLVM}
    \label{LLVM}

    Building binaries themselves is unfortunately not enough, since KLEE does not take pure binaries as its input, but instead requires LLVM bytecode. Compiling an ordinary \lstinline{.c} file to LLVM can easily be done using \lstinline{clang}. However, again, coreutils use a complex build system which means to just use \lstinline{clang}, I'd have to deeply understand and modify it, with the drawbacks listed above.

    Simply passing \lstinline{clang} as the C compiler to the build system does not work, since the produced output is not a runnable binary, and the build system requires the compiler's output to be executable.

    Fortunately, there exists Whole Program LLVM (WLLVM)\cite{WLLVM}, a tool specifically designed to work with complex build systems while still producing LLVM bytecode as one of its outputs. This is achieved by injecting its compiler into the build system. The compiler creates executable binaries and additionally injects LLVM bytecode into a dedicated section of the object files. In a second step, these then get extracted and linked together to produce LLVM bytecode files.

    Since I'm running WLLVM on an old version of Ubuntu, I was forced to use an old version of WLLVM as well, because newer versions require a version of python which is not available on Ubuntu 14.04. To create proper input files for KLEE, I added two options, to reduce warnings (\lstinline{--build}) and to turn off premature optimizations according to the KLEE documentation (\lstinline{CFLAGS})\cite{KLEETutorial}.

    The Dockerfile section to build the LLVM bytecode can be found in Listing~\ref{dockerfile2}.

    \subsubsection{Coverage Data Gathering}
    \label{gcov}

    A simple way to compare what these experiments accomplish compared to the experiments documented in the original paper is to look at coverage data, specifically coverage as measured by \lstinline{gcov}. To gather this information, one needs to compile the binaries using GCC, and tell the compiler to add coverage gathering instrumentation. Along with the binaries, a note document (\lstinline{<executable-name>.gcno}) is created. When the binary is executed, the added instrumentation records which path through the code is taken and, together

\end{multicols}
\lstinputlisting[
    language=docker,
    firstline=56,
    lastline=70,
    consecutivenumbers=false,
    caption=Dockerfile content to build coreutils instrumented to record coverage,
    label=dockerfile3
]{../Dockerfile}
\begin{multicols}{2}

    with information from the notes file, stores its results in a coverage data file (\lstinline{<executable-name>.gcda}). This file can then be analyzed with \lstinline{gcov} to get human-readable coverage data.

    With this step however, I ran into the same issue as before: Recent versions of GCC no longer build coreutils 6.10. I adopted the same approach and used the same base image as described in Section~\ref{OldVersion}. The Dockerfile excerpt with the build step can be found in Listing~\ref{dockerfile3}.

    I made two changes compared to building the LLVM bytecode files, to increase the accuracy of the measurements:
    \begin{itemize}
        \item I replaced all calls to \lstinline{_exit} with calls to \lstinline{exit}, so that those instructions are also included in the measurements. This was done according to the instructions in the FAQ\cite{KLEEFAQ}.
        \item The original paper mentions that coverage is measured only on executable lines of code. Specifically, Section 5.1 of the original paper says
              \begin{displayquote}
                  We measure size in terms of executable lines if code (ELOC) by counting the total number of executable lines in the final executable after global optimization, which eliminates uncalled functions and other dead code.\cite{KLEE}
              \end{displayquote}
              I am not sure how \citeauthor{KLEE} calculated the executable lines of code, since this is not trivial. I did enable normal global optimization (\lstinline{-O2}), but this may still result in a considerable underestimation of coverage.
    \end{itemize}


    \subsubsection{Preparing KLEE}

    Finally, the bytecode files can be passed to KLEE for the actual fuzzing. To prepare KLEE's Docker image, the environment and sandbox are prepared according to the documentation\cite{KLEEFAQ}. Then, the bytecode files from the step outlined in Section~\ref{LLVM} and the binaries instrumented with \lstinline{gcov} along with their notes files are copied to the analysis image. The analysis step itself is an involved process itself and is done by executing a shell script (\lstinline{analyze.sh}). This step is explained in Section~\ref{analyze.sh}. The analysis script is copied into the image and executed on container start. To allow passing certain settings to the analysis step, environment variables are used,which can be set in the \lstinline{docker run} command.

    The Dockerfile excerpt for this step can be found in Listing~\ref{dockerfile4}.

\end{multicols}
\lstinputlisting[
    language=docker,
    firstline=84,
    lastline=117,
    consecutivenumbers=false,
    caption=Dockerfile content to prepare the fuzzing stage,
    label=dockerfile4
]{../Dockerfile}
\begin{multicols}{2}

    \subsubsection{Running KLEE}
    \label{analyze.sh}
    When starting the Docker image built with the steps outlined before, a shell script is executed. This script handles the evaluation settings, input and output files, and collects metrics. Specifically, the following steps are performed:

    \begin{enumerate}
        \item The input including the passed settings are parsed. The script allows setting input and output directories (\lstinline{--llvm-dir}, \lstinline{--cov-dir}, \lstinline{--out-dir}), KLEE's timeout (\lstinline{--klee-max-time}), and skipping the fuzzing step (\lstinline{--skip-klee-analysis}). The latter allows gathering additional metrics without based on the output from a previous fuzzing run without having to perform additional, computationally expensive analysis.
        \item To run KLEE, the analyst is required to pass arguments setting the size and number of inputs and input files to be tested. For most coreutils, this is the same, however (as mentioned in Section 5.2 of the original paper\cite{KLEE} and explained in the FAQs\cite{KLEEFAQ}) some utils need different settings to achieve a decent coverage. The analysis script assembles the command to run KLEE, including the constant settings, util-dependant settings, and the timeout set in the script arguments.
        \item Then, the actual fuzzing is performed.
        \item KLEE's output is examined in a few ways:\begin{enumerate}
                  \item For each found error, human readable outputs are created using \lstinline{ktest-tool}.
                  \item \lstinline{klee-stats} is invoked to export metrics collected by KLEE.
                  \item All test cases generated by KLEE are used as input for \lstinline{klee-replay} pointed at the binary instrumented with coverage gathering instructions. This ensures that each instruction analyzed by KLEE during its fuzzing is also executed and thus recorded in the coverage results. Since the instrumented binaries were not compiled on the same system as they are executed on, the environment variables \lstinline{GCOV_PREFIX} and \lstinline{GCOV_PREFIX_STRIP} need to be set appropriately.
              \end{enumerate}
        \item Finally, certain large output text files are compressed to minimize disk usage.
    \end{enumerate}

    \subsubsection{Extracting Human-Readable Coverage Data}

    As a last step, the output of the binaries instrumented to gather coverage metrics needs to be fed back into \lstinline{gcov}. Unfortunately, the format of these output files changed at some point and the version of \lstinline{gcov} installed in KLEE's Docker image is no longer able to read them. They are therefore fed back into the Docker image that created them, where an obviously compatible version of \lstinline{gcov} is available.

    \pagebreak

    \subsection{Analyzing a Single Run}
    \subsubsection{Gathered Metrics}
    \subsubsection{Comparison to the Original Paper}

    \section{Comparing Runs}
    \label{Timeouts}
    \subsection{Metrics Distribution}
    \subsection{Influence of Testing Timeout}

    \section{Testing More Recent Versions of coreutils}
    \label{SoftwareEvolution}
    \subsection{Differences in Testing Setup}
    \subsection{Findings}

    \section{Discussion}
    \subsection{Research Questions}
    \subsection{Produced Artifacts}
    \subsection{Future Work}

    \columnbreak
    \defbibheading{bibliography}[\bibname]{\section*{#1}}
    \addcontentsline{toc}{section}{\bibname}
    \printbibliography

    \renewcommand{\thesubsection}{\arabic{subsection}}
    \renewcommand{\thesubsubsection}{\arabic{subsection}.\arabic{subsubsection}}
    \setcounter{subsection}{0}
    \setcounter{subsubsection}{0}
    \section*{Appendix}
    \addcontentsline{toc}{section}{Appendix}
    \subsection{Error When Attempting to Build coreutils 6.10 on KLEE's Docker image}
    \label{AppendixErrorBuildOldInNew}
    \subsubsection{Error}
    \lstinputlisting[firstline=1130, consecutivenumbers=false]{assets/build-coreutils-6.10-on-klee-image.txt}
    \subsubsection{\texorpdfstring{\lstinline{freadahead.c}}{freadahead.c}}
    The license has been cut for brevity's sake.
    \lstinputlisting[firstline=17, language=C, consecutivenumbers=false]{assets/freadahead.c}

\end{multicols}

\end{document}
